"""
Statistical Significance Test (Welch's t-test)
==============================================

Description:
    This script compares the performance of Deep Learning models (e.g., DAF-MoE, FT-Transformer)
    against Gradient Boosted Decision Tree (GBDT) baselines (e.g., XGBoost, CatBoost).
    
    It parses the evaluation logs (JSON files) from multiple random seeds and performs
    Welch's t-test (unequal variances) to determine statistical significance.

    The results, including p-values and win/loss verdicts, are printed to the console
    and saved as a CSV file for reporting in the paper.

Prerequisites:
    1. Evaluation logs (JSON) must exist in the `results/scores/` directory.
       - These files are generated by running `train.py` or `runners/run_trees.py`.
    2. Expected filename format: `{Dataset}_{Model}_seed{seed}.json`.
    3. At least 2 seeds per model/dataset are required to calculate variance and p-values.

Usage:
    python analysis/compare_baselines.py
"""

import os
import json
import numpy as np
import pandas as pd
import glob
from scipy import stats
from collections import defaultdict

# ==========================================
# 1. Custom Metric Rules (Exact Match)
# ==========================================
# Define the primary evaluation metric for each dataset.
METRIC_RULES = {
    "Adult Census Income": "acc",
    "Higgs small": "acc",
    "Covertype": "acc",
    "BNP Paribas": "acc",
    "NHANES": "acc",
    "Credit Card Fraud": "auprc",
    "MIMIC-III Mortality": "auprc",
    "MIMIC-IV Mortality": "auprc",
    "California Housing": "rmse",
    "YearPrediction": "rmse",
    "Allstate": "rmse"
}

def get_metric_value(metrics_data, target_metric):
    """
    Safely extracts the target metric value from the metrics dictionary or list.
    
    Args:
        metrics_data (dict or list): The 'metrics' field from the JSON log.
        target_metric (str): The key of the metric to extract (e.g., 'acc', 'rmse').
        
    Returns:
        float or None: The metric value if found, else None.
    """
    if isinstance(metrics_data, list):
        if len(metrics_data) > 0:
            return metrics_data[0].get(target_metric)
    elif isinstance(metrics_data, dict):
        return metrics_data.get(target_metric)
    return None

def main():
    # Define paths assuming execution from project root directory
    score_dir = "results/scores"
    output_dir = "results/analysis"
    
    # Ensure output directory exists
    os.makedirs(output_dir, exist_ok=True)
    
    # Locate all JSON log files
    files = glob.glob(os.path.join(score_dir, "*.json"))
    
    # Define model categories for comparison
    # DL_MODELS include our proposed method (DAF-MoE) and other deep baselines
    DL_MODELS = ['daf_moe', 'daf-moe', 'ft_transformer', 'mlp', 'resnet']
    GBDT_MODELS = ['xgboost', 'catboost']
    
    # Dictionary to store scores: data_map[dataset][model] = [score_seed1, score_seed2, ...]
    data_map = defaultdict(lambda: defaultdict(list))
    
    print(f"üìÇ Found {len(files)} evaluation logs. Parsing...")

    for fpath in files:
        try:
            filename = os.path.basename(fpath)
            
            # 1. Extract Dataset Name (assumes format: "DatasetName_ModelName_...")
            dataset_key = filename.split('_')[0]
            
            # 2. Determine Target Metric
            target_metric = METRIC_RULES.get(dataset_key)
            
            if not target_metric:
                # Default to accuracy if dataset is not explicitly defined
                target_metric = 'acc' 

            with open(fpath, 'r', encoding='utf-8') as f:
                content = json.load(f)
            
            # 3. Extract Model Name (normalize to lowercase)
            model_raw = content.get('model', 'Unknown').lower()
            if "daf" in model_raw: model_key = "daf_moe"
            elif "ft" in model_raw and "trans" in model_raw: model_key = "ft_transformer"
            else: model_key = model_raw
            
            # 4. Extract Score
            metrics_data = content.get('metrics', {})
            score = get_metric_value(metrics_data, target_metric)
            
            if score is not None:
                data_map[dataset_key][model_key].append(score)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error parsing {filename}: {e}")
            continue

    # --- Statistical Significance Test (Welch's t-test) ---
    results = []
    print("\nüîç Running Welch's t-tests (Deep Learning vs GBDT)...")

    for dataset, models_data in data_map.items():
        metric_name = METRIC_RULES.get(dataset, 'ACC').upper()
        is_lower_better = (metric_name == 'RMSE')
        
        # Identify Deep Learning candidates
        dl_candidates = [m for m in models_data.keys() if m in DL_MODELS or "daf" in m]
        
        for dl_model in dl_candidates:
            scores_dl = models_data[dl_model]
            # Need at least 2 samples to calculate variance
            if len(scores_dl) < 2: continue
            
            # Identify GBDT candidates
            gbdt_candidates = [m for m in models_data.keys() if m in GBDT_MODELS]
            
            for gbdt_model in gbdt_candidates:
                scores_gbdt = models_data[gbdt_model]
                if len(scores_gbdt) < 2: continue
                
                # Perform Welch's t-test (independent samples, unequal variance)
                t_stat, p_val = stats.ttest_ind(scores_dl, scores_gbdt, equal_var=False)
                
                mean_dl = np.mean(scores_dl)
                mean_gbdt = np.mean(scores_gbdt)
                
                # Determine Verdict (Win/Loss/Tie)
                if p_val >= 0.05:
                    verdict = "Tie (Insignificant)"
                else:
                    if is_lower_better:
                        verdict = "Win (DL)" if mean_dl < mean_gbdt else "Loss (GBDT)"
                    else:
                        verdict = "Win (DL)" if mean_dl > mean_gbdt else "Loss (GBDT)"
                
                # Highlight DAF-MoE for better readability in the table
                model_display = dl_model.upper()
                if "DAF" in model_display: model_display = "** " + model_display
                
                results.append({
                    "Dataset": dataset,
                    "Metric": metric_name,
                    "DL Model": model_display,
                    "GBDT Model": gbdt_model.upper(),
                    "p-value": p_val,
                    "Verdict": verdict,
                    "Mean (DL)": mean_dl,
                    "Mean (GBDT)": mean_gbdt
                })

    if results:
        df = pd.DataFrame(results)
        # Sort by Dataset then Model for structured output
        df = df.sort_values(by=["Dataset", "DL Model"])
        
        # Format p-value for clean display (scientific notation for very small values)
        df["p-val"] = df["p-value"].apply(lambda x: f"{x:.4f}" if x >= 0.001 else f"{x:.2e}")
        
        print("\n" + "="*95)
        print("üìä Statistical Significance Test Results (p > 0.05 implies Tie)")
        print("="*95)
        print(df[["Dataset", "Metric", "DL Model", "GBDT Model", "p-val", "Verdict"]].to_string(index=False))
        
        # Save to CSV
        output_csv_path = os.path.join(output_dir, "dl_vs_gbdt_final.csv")
        df.to_csv(output_csv_path, index=False)
        print(f"\nüíæ Saved result summary to {output_csv_path}")
    else:
        print("‚ö†Ô∏è No valid comparison pairs found. Check filenames or METRIC_RULES keys.")

if __name__ == "__main__":
    main()